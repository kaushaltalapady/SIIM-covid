{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:09:17.170484Z","iopub.execute_input":"2021-08-07T16:09:17.171487Z","iopub.status.idle":"2021-08-07T16:13:29.897604Z","shell.execute_reply.started":"2021-08-07T16:09:17.171442Z","shell.execute_reply":"2021-08-07T16:13:29.896535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import apex\n!pip install albumentations==0.4.6\n!pip install effdet\n!pip install timm\n!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:13:29.901858Z","iopub.execute_input":"2021-08-07T16:13:29.902256Z","iopub.status.idle":"2021-08-07T16:13:54.633575Z","shell.execute_reply.started":"2021-08-07T16:13:29.902215Z","shell.execute_reply":"2021-08-07T16:13:54.632537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom effdet import create_model\nfrom pathlib import Path\nfrom tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:13:54.63686Z","iopub.execute_input":"2021-08-07T16:13:54.637269Z","iopub.status.idle":"2021-08-07T16:13:54.646495Z","shell.execute_reply.started":"2021-08-07T16:13:54.637227Z","shell.execute_reply":"2021-08-07T16:13:54.645541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = 768\n\n# number of fold that we are going to choose for validation during the training\nfold_number = 0\n\n# format of our images\nimage_ext = 'jpg'\n\n# path to images directory\nTRAIN_ROOT_PATH = 'train'\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:13:54.648122Z","iopub.execute_input":"2021-08-07T16:13:54.648945Z","iopub.status.idle":"2021-08-07T16:13:54.655514Z","shell.execute_reply.started":"2021-08-07T16:13:54.648904Z","shell.execute_reply":"2021-08-07T16:13:54.654369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir train","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:13:54.657201Z","iopub.execute_input":"2021-08-07T16:13:54.657842Z","iopub.status.idle":"2021-08-07T16:13:55.386756Z","shell.execute_reply.started":"2021-08-07T16:13:54.657804Z","shell.execute_reply":"2021-08-07T16:13:55.385808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -xvf ../input/jpg-data/train.tar.gz -C train/","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:13:55.388576Z","iopub.execute_input":"2021-08-07T16:13:55.388852Z","iopub.status.idle":"2021-08-07T16:14:01.151396Z","shell.execute_reply.started":"2021-08-07T16:13:55.388822Z","shell.execute_reply":"2021-08-07T16:14:01.150454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_files_in_folder(folder, types):\n    files_grabbed = []\n    for t in types:\n        files_grabbed.extend(folder.rglob(t))\n    files_grabbed = sorted(files_grabbed, key=lambda x: x)\n    return files_grabbed\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:14:01.152988Z","iopub.execute_input":"2021-08-07T16:14:01.153319Z","iopub.status.idle":"2021-08-07T16:14:01.160064Z","shell.execute_reply.started":"2021-08-07T16:14:01.153289Z","shell.execute_reply":"2021-08-07T16:14:01.159089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef prepare_csv_for_efdet(input_filepath, output_filepath,SIZE=768):\n    frame=pd.read_csv('../input/covid-kaushal/new_train_boxes.csv')\n    train_image_df = pd.read_csv(input_filepath)\n    train_image_df['id'] = train_image_df['id'].str.split('_', expand=True)[0]\n\n    image_ids = train_image_df['id'].tolist()\n    labels_raw = train_image_df['label'].tolist()\n    boxes_raw = train_image_df['boxes'].tolist()\n\n    images = get_all_files_in_folder(Path('train'), ['*.jpg'])\n\n    result = []\n    for image_path in tqdm(images, colour = '#00ff00'):\n        s = image_path.stem + ','\n\n        boxes = []\n        for image_id, label in zip(image_ids, labels_raw):\n            if image_id == image_path.stem:\n                w,h=frame[frame['image_id']==image_id][['w','h']].iloc[0]\n                label_split = label.split('opacity')\n                if len(label_split) > 1:\n\n                    for l in label_split:\n                        if l != '':\n                            box = l.split(' ')\n                            x1 = (float(box[2])/w)*SIZE\n                            if x1 < 0: x1 = 0.0\n                            y1 = (float(box[3])/h)*SIZE\n                            if y1 < 0: y1 = 0.0\n                            x2 = (float(box[4])/w)*SIZE\n                            y2 = (float(box[5])/w)*SIZE\n\n                            boxes.append([x1, y1, x2, y2])\n\n        boxes_str = ''\n        if len(boxes):\n            for box in boxes:\n                boxes_str += str(box[0]) + ' ' + str(box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ';'\n\n            s += boxes_str[:-1]\n        else:\n            boxes_str = 'no_box'\n            s += boxes_str\n\n        s += ',0'\n        result.append(s)\n\n    with open(output_filepath, 'w') as f:\n        f.write('image_name,BoxesString,domain\\n')\n        for item in result:\n            f.write(\"%s\\n\" % item)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:14:01.16241Z","iopub.execute_input":"2021-08-07T16:14:01.163415Z","iopub.status.idle":"2021-08-07T16:14:01.183714Z","shell.execute_reply.started":"2021-08-07T16:14:01.163374Z","shell.execute_reply":"2021-08-07T16:14:01.182795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_filepath = '/kaggle/input/siim-covid19-detection/train_image_level.csv'\noutput_filepath = '/kaggle/working/images_train.csv'\nprepare_csv_for_efdet(input_filepath, output_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:14:01.18547Z","iopub.execute_input":"2021-08-07T16:14:01.186462Z","iopub.status.idle":"2021-08-07T16:15:08.788254Z","shell.execute_reply.started":"2021-08-07T16:14:01.186422Z","shell.execute_reply":"2021-08-07T16:15:08.78724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frame=pd.read_csv('../input/covid-kaushal/new_train_boxes.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.789964Z","iopub.execute_input":"2021-08-07T16:15:08.790352Z","iopub.status.idle":"2021-08-07T16:15:08.834366Z","shell.execute_reply.started":"2021-08-07T16:15:08.790295Z","shell.execute_reply":"2021-08-07T16:15:08.833266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df =pd.read_csv('./images_train.csv')\ndf.drop(df[df.BoxesString == 'no_box'].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.836101Z","iopub.execute_input":"2021-08-07T16:15:08.836458Z","iopub.status.idle":"2021-08-07T16:15:08.865871Z","shell.execute_reply.started":"2021-08-07T16:15:08.836419Z","shell.execute_reply":"2021-08-07T16:15:08.864895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ndf_folds = df[['image_name']].copy()\nfor fold_number, (train_index, val_index) in enumerate(skf.split(X=df.image_name, y=df['domain'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.867856Z","iopub.execute_input":"2021-08-07T16:15:08.868211Z","iopub.status.idle":"2021-08-07T16:15:08.88775Z","shell.execute_reply.started":"2021-08-07T16:15:08.868176Z","shell.execute_reply":"2021-08-07T16:15:08.886752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([\n                       \n                        # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5), \n                        ToTensorV2(p=1.0), \n                        ],\n                        p=1.0,\n                        bbox_params=A.BboxParams(\n                                                format=\"pascal_voc\",\n                                                min_area=0, \n                                                min_visibility=0,\n                                                label_fields=['labels']\n                    )\n    )\n\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=img_size, width=img_size, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.889611Z","iopub.execute_input":"2021-08-07T16:15:08.889993Z","iopub.status.idle":"2021-08-07T16:15:08.898592Z","shell.execute_reply.started":"2021-08-07T16:15:08.889956Z","shell.execute_reply":"2021-08-07T16:15:08.89765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        index = self.image_ids[index]\n        image_name = self.marking.loc[index]['image_name']\n\n        # if self.test or random.random() > 0.3:\n        image, boxes = self.load_image_and_boxes(index)\n        # else:\n            # image, boxes = self.load_cutmix_image_and_boxes(index)\n\n        # there is only one class\n        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index])}\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                print(sample)\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:, [0, 1, 2, 3]] = target['boxes'][:, [1, 0, 3, 2]]  # yxyx: be warning\n                    target['labels'] = target['labels'][:len(target['boxes'])]\n                    break\n\n        return image, target, image_name\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_name = self.marking['image_name'][index]\n        image = cv2.imread(TRAIN_ROOT_PATH+'/'+image_name + '.' + image_ext, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        row = self.marking.loc[index]\n\n        bboxes = []\n        if row['BoxesString'] != 'no_box':\n            for bbox in row['BoxesString'].split(';'):\n                bboxes.append(list(map(float, bbox.split(' '))))\n        return image, np.array(bboxes)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.90079Z","iopub.execute_input":"2021-08-07T16:15:08.901357Z","iopub.status.idle":"2021-08-07T16:15:08.922199Z","shell.execute_reply.started":"2021-08-07T16:15:08.90132Z","shell.execute_reply":"2021-08-07T16:15:08.921034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create datasets using fold_number\ntrain_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] != fold_number].index.values,\n    marking=df,\n    transforms=get_train_transforms(),\n    test=False,\n)\n\nvalidation_dataset = DatasetRetriever(\n    image_ids=df_folds[df_folds['fold'] == fold_number].index.values,\n    marking=df,\n    transforms=get_valid_transforms(),\n    test=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.924295Z","iopub.execute_input":"2021-08-07T16:15:08.924746Z","iopub.status.idle":"2021-08-07T16:15:08.936128Z","shell.execute_reply.started":"2021-08-07T16:15:08.924709Z","shell.execute_reply":"2021-08-07T16:15:08.935096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, target, image_id = train_dataset[1]\n\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\nlabels = target['labels'].cpu().numpy().astype(np.int32)\n\nnumpy_image = image.permute(1,2,0).cpu().numpy()\nnumpy_image_box = numpy_image.copy()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(numpy_image_box, (int(box[1]), int(box[0])), (int(box[3]),  int(box[2])), (0, 1, 0), 2)\n\nax.set_axis_off()\nax.imshow(numpy_image_box)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:08.937522Z","iopub.execute_input":"2021-08-07T16:15:08.938242Z","iopub.status.idle":"2021-08-07T16:15:09.386123Z","shell.execute_reply.started":"2021-08-07T16:15:08.938206Z","shell.execute_reply":"2021-08-07T16:15:09.385263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:09.388312Z","iopub.execute_input":"2021-08-07T16:15:09.389424Z","iopub.status.idle":"2021-08-07T16:15:09.397469Z","shell.execute_reply.started":"2021-08-07T16:15:09.389376Z","shell.execute_reply":"2021-08-07T16:15:09.396361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Fitter:\n    \n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ] \n\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            summary_loss = self.validation(validation_loader)\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            if summary_loss.avg < self.best_summary_loss:\n                self.best_summary_loss = summary_loss.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        summary_loss = AverageMeter()\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            # if self.config.verbose:\n            #     if step % self.config.verbose_step == 0:\n            #         print(\n            #             f'Val Step {step}/{len(val_loader)}, ' + \\\n            #             f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n            #             f'time: {(time.time() - t):.5f}', end='\\r'\n            #         )\n\n            print(\n                f'Val Step {step}/{len(val_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'time: {(time.time() - t):.5f}', end='\\r'\n            )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                boxes = [target['boxes'].to(self.device).float() for target in targets]\n                labels = [target['labels'].to(self.device).float() for target in targets]\n\n                target_res = {}\n                target_res['bbox'] = boxes\n                target_res['cls'] = labels \n                target_res[\"img_scale\"] = torch.tensor([1.0] * batch_size, dtype=torch.float).to(self.device)\n                target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * batch_size, dtype=torch.float).to(self.device)\n\n                outputs = self.model(images, target_res)\n                loss = outputs['loss']\n                summary_loss.update(loss.detach().item(), batch_size)\n\n        return summary_loss\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n        t = time.time()\n        \n        #apex\n        scaler = torch.cuda.amp.GradScaler()\n\n\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            # if self.config.verbose:\n            #     if step % self.config.verbose_step == 0:\n            #         print(\n            #             f'Train Step {step}/{len(train_loader)}, ' + \\\n            #             f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n            #             f'time: {(time.time() - t):.5f}', end='\\r'\n            #         )\n\n            print(\n                f'Train Step {step}/{len(train_loader)}, ' + \\\n                f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                f'time: {(time.time() - t):.5f}', end='\\r'\n            )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            \n            target_res = {}\n\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n\n            target_res['bbox'] = boxes\n            target_res['cls'] = labels \n\n            \n            self.optimizer.zero_grad()\n\n            #apex\n            with torch.cuda.amp.autocast():\n                outputs = self.model(images, target_res)\n            \n            loss = outputs['loss']\n\n            #apex\n            scaler.scale(loss).backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            #apex\n            scaler.step(self.optimizer)\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n            #apex\n            scaler.update()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:09.399742Z","iopub.execute_input":"2021-08-07T16:15:09.400917Z","iopub.status.idle":"2021-08-07T16:15:09.44843Z","shell.execute_reply.started":"2021-08-07T16:15:09.400874Z","shell.execute_reply":"2021-08-07T16:15:09.447517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 2\n    n_epochs = 1\n    lr = 0.0002\n\n    # folder where we are going to save weights\n    folder = '/output/fold0'\n\n    verbose = True\n    verbose_step = 1\n    \n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = True  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=5,\n        verbose=True, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:09.450538Z","iopub.execute_input":"2021-08-07T16:15:09.451711Z","iopub.status.idle":"2021-08-07T16:15:09.461419Z","shell.execute_reply.started":"2021-08-07T16:15:09.45167Z","shell.execute_reply":"2021-08-07T16:15:09.460541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=False,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    \n    # Attention! Add this line if you want to continue training with your weights\n    # fitter.load('/content/gdrive/MyDrive/datasets/sber_food/efficientdet/fold0/last-checkpoint.bin')\n    fitter.fit(train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:09.463466Z","iopub.execute_input":"2021-08-07T16:15:09.464669Z","iopub.status.idle":"2021-08-07T16:15:09.475102Z","shell.execute_reply.started":"2021-08-07T16:15:09.464631Z","shell.execute_reply":"2021-08-07T16:15:09.473811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from effdet import create_model_from_config, get_efficientdet_config\ndevice = 'cuda'\ndef get_net():\n    config = get_efficientdet_config('tf_efficientdet_d6')\n\n    config.image_size = [img_size,img_size]\n    config.norm_kwargs=dict(eps=.001, momentum=.01)\n\n    net = EfficientDet(config, pretrained_backbone=False)\n    checkpoint = torch.load('/kaggle/input/efficientdet-init-weights/efficientdet_d6-51cb0132.pth')\n    net.load_state_dict(checkpoint)\n\n    # we have only one class - opacity\n    net.reset_head(num_classes=1)\n    net.class_net = HeadNet(config, num_outputs=config.num_classes)\n\n    return DetBenchTrain(net, config)\n\nnet = get_net()\nnet.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:15:09.477364Z","iopub.execute_input":"2021-08-07T16:15:09.477814Z","iopub.status.idle":"2021-08-07T16:15:11.800469Z","shell.execute_reply.started":"2021-08-07T16:15:09.477774Z","shell.execute_reply":"2021-08-07T16:15:11.799663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_training():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=RandomSampler(train_dataset),\n        pin_memory=False,\n        drop_last=False,\n        num_workers=TrainGlobalConfig.num_workers,\n        collate_fn=collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    \n    # Attention! Add this line if you want to continue training with your weights\n    # fitter.load('/content/gdrive/MyDrive/datasets/sber_food/efficientdet/fold0/last-checkpoint.bin')\n    fitter.fit(train_loader, val_loader)\nrun_training()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T16:16:23.87183Z","iopub.execute_input":"2021-08-07T16:16:23.872268Z"},"trusted":true},"execution_count":null,"outputs":[]}]}